\section{Setup}

\( \boldsymbol{\xi} \to \) Cartesian position (2D/3D) of the agent, \( \boldsymbol{\xi} \in \mathbb{R}^2 \) or \( \mathbb{R}^3 \).

\( \varphi \to \) Progress value, \( \varphi \in [0, 1] \).

\( \boldsymbol{x} \to \) State of the system, \( \boldsymbol{x} \coloneq [\boldsymbol{\xi}, \varphi] \).

\( \boldsymbol{d}_n \to \) \( n \)-th demonstration, which is a set of \( l \) sequential states, \( \boldsymbol{d}_n = \{ \boldsymbol{x}_{n, 0}, \ldots, \boldsymbol{x}_{n, l} \} \).

\( N \to \) Total number of demonstrations, \( N \in \mathbb{N} \).

\( \mathcal{D}^{(0)} \to \) Set of all given training demonstrations, in the global (fixed) frame \( \{ 0 \} \), \( \mathcal{D}^{(0)} = \{ {}^{0}\boldsymbol{d}_0, \ldots, {}^{0}\boldsymbol{d}_N \} \).

\( M \to \) Total number of frames, \( M \in \mathbb{N} \).

\subsection{Transformation}

For each frame \( m \), the datapoint \( {}^{0}\boldsymbol{x}_{n, i} \) is tranformed from frame \( 0 \) to frame \( m \) by

\begin{equation}
    {}^{m}\boldsymbol{x}_{n, i}
    =
    \begin{bmatrix}
        {}^{m}\boldsymbol{t} \\
        0
    \end{bmatrix}
    +
    {}^{m}\boldsymbol{H} \; {}^{0}\boldsymbol{x}_{n, i}
    , \quad
    \text{where }
    {}^{m}\boldsymbol{H}
    =
    \begin{bmatrix}
        {}^{m}\boldsymbol{R} & \boldsymbol{0} \\
        \boldsymbol{0}       & 1
    \end{bmatrix}
    , \quad \forall \
    \begin{aligned}
        i & \in \{ 0, \ldots, l \} \\
        n & \in \{ 0, \ldots, N \} \\
        m & \in \{ 1, \ldots, M \}
    \end{aligned}
\end{equation}

where \( \big \{ \)
\( {}^{m}\boldsymbol{t} \to \) Translation vector, \( {}^{m}\boldsymbol{R} \to \) Rotation matrix \( \big \} \) from frame \( 0 \) to frame \( m \).

\( \mathcal{D}^{(m)}, m \in \{ 1, \ldots, M \} \to \) \( m \)-th transformed demonstration set, starting from \( {}^{0}\boldsymbol{d}_n, \forall n \in \{ 0, \ldots, N \} \), and transformed to frame \( m \) using the transformation above, giving \( \mathcal{D}^{(m)} = \{ {}^{m}\boldsymbol{d}_0, \ldots, {}^{m}\boldsymbol{d}_N \} \).

Inputs to the proposed method \( \to \mathcal{D}^{(0)}, \mathcal{D}^{(1)}, \ldots, \mathcal{D}^{(M)} \).

Final result \( \to \) A policy, taking inputs as the current positions relative to each frame and the current progress value \( \varphi \), and outputs a desired change in the state \( \Delta \boldsymbol{x} \).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/images/tpgp-pipeline.jpeg}
    \caption{
        TPGP pipeline
    }\label{fig:tpgp-pipeline}
\end{figure}

\subsection{Alignment of demonstrations}

For each \( i \)-th demonstration in \( \mathcal{D}^{(m)} \), find the index \( h \) of the closest point \( {}^{m}\boldsymbol{x}_{i, \cdot} \) to every other element of other demonstrations \( {}^{m}\boldsymbol{x}_{j, \cdot} \), giving
\begin{equation}
    {}^{m}\boldsymbol{A}_{ij} = \argmin_{h} \Vert {}^{m}\boldsymbol{x}_{i, h} - {}^{m}\boldsymbol{x}_{j, \cdot} \Vert
    , \qquad
    {}^{m}\boldsymbol{B}_{ij} = \varphi_{i, A_{ij}}
    , \qquad
    i, j \in \{ 0, \ldots, N \}
\end{equation}

\textit{Assumption:} In each of the local frames, the closest point in space must have happened at the same (normalized) time.
Don't use dynamic time warping (DTW).

\( P_{im} \to \) Keypoint progress values for demonstration \( i \) in frame \( m \).
\begin{equation}
    P_{im} = \operatorname{median}({}^{m}\boldsymbol{B}_{i\cdot}), \quad \forall i \in \{ 0, \ldots, N \}, \forall m \in \{ 1, \ldots, M \}
\end{equation}

\( \mathcal{D}^{(0*)}, \mathcal{D}^{(1*)}, \ldots, \mathcal{D}^{(M*)} \to \) Aligned (and transformed) demonstration sets.

\subsection{Training of local policies}

Local policies are encoded as a dynamical system, which are learnt using Gaussian processes regression with a Mat\'{e}rn kernel and zero mean prior.
\begin{equation}
    \Delta\boldsymbol{x} = f(\boldsymbol{x})
    , \qquad
    f(\boldsymbol{x}) \sim \mathcal{GP}(0, k(\boldsymbol{x}, \boldsymbol{x}'))
\end{equation}

A variational approximation of the posterior distribution is used,
\begin{equation}
    q(\boldsymbol{u}) = \mathcal{N}(\boldsymbol{u} \mid \boldsymbol{m}, \boldsymbol{S}), \quad \text{where } \boldsymbol{u} \to \text{A set of inducing variables in } \boldsymbol{Z}
\end{equation}
and the predictive distribution on a test point \( \boldsymbol{X}_* \) is
\begin{align}
    p(\boldsymbol{f}_*)
     & \coloneq
    \int p(\boldsymbol{f}_* \mid \boldsymbol{u}) \; q(\boldsymbol{u}) \, d\boldsymbol{u}
    \\
    \implies
    p(\boldsymbol{f}_*)
     & =
    \mathcal{N} \Big( \boldsymbol{A} \boldsymbol{m}, \boldsymbol{K}(\boldsymbol{X}_*, \boldsymbol{X}_*) + \boldsymbol{A} \big( \boldsymbol{S} - \boldsymbol{K}(\boldsymbol{Z}, \boldsymbol{Z}) \big) \boldsymbol{A}^\top \Big)
    \\
    \text{where }
    \boldsymbol{A}
     & =
    \boldsymbol{K}(\boldsymbol{X}_*, \boldsymbol{Z}) \ {\boldsymbol{K}(\boldsymbol{Z}, \boldsymbol{Z})}^{-1}
\end{align}

\subsubsection{Mat\'{e}rn kernel}

\begin{equation}
    k(x_i, x_j)
    =
    \frac{1}{\Gamma(\nu) 2^{\nu - 1}} {\left( \frac{\sqrt{2\nu}}{l} d(x_i , x_j) \right)}^\nu K_\nu \left( \frac{\sqrt{2\nu}}{l} d(x_i , x_j) \right)
\end{equation}

\( \nu \to \) Smoothness parameter, \( l \to \) Length scale, \( d(\cdot, \cdot) \to \) Euclidean distance, \( \Gamma(\cdot) \to \) Gamma function, \( K_\nu(\cdot) \to \) Modified Bessel function of the second kind.

\( \nu \to \infty \implies \) RBF kernel, \( \nu = 1/2 \implies \) Absolute exponential kernel.

\subsection{Frame relevance GP}

\begin{equation}
    \boldsymbol{\alpha} \sim \mathcal{GP}(0, k(\varphi, \varphi'))
\end{equation}

Weighted sum of predicted transitions for each point in the demonstration:
\begin{align}
    p \left( {}^{0}f_i \right)
     & =
    \mathcal{N} \left( {}^{0}\mu_i, {}^{0}\boldsymbol{\Sigma}_i \right)
    \\
    {}^{0}\mu_i
     & =
    \sum_{m = 1}^M {}^{m}\alpha_i \ {}^{m}\boldsymbol{H}^{-1} \ {}^{m}\mu_i \left( {}^{m}x_i \right)
    \\
    {}^{0}\boldsymbol{\Sigma}_i
     & =
    \sum_{m = 1}^M {}^{m}\alpha_i \ {}^{m}\boldsymbol{H}^{-1} \ {}^{m}\boldsymbol{\Sigma}_i \ {}^{m}\boldsymbol{H}
\end{align}

\( {}^{0}\Delta\boldsymbol{x} \to \) Recorded desired transition

Maximising the likelihood of each label transition to belong to the predicted distribution \( p \left( {}^{0}f_i \right) \):
\begin{equation}
    p \left( {}^{0}\Delta\boldsymbol{x} \mid {}^{0}\boldsymbol{f} \right)
    =
    \prod_{i}^{n_d} p \left( {}^{0}\Delta\boldsymbol{x}_i \mid {}^{0}f_i \right)
\end{equation}
